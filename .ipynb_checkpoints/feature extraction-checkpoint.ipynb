{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from os import walk\n",
    "\n",
    "from scipy.io import loadmat\n",
    "# import scipy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 240000\n"
     ]
    }
   ],
   "source": [
    "num_ch = 0 #number of channels\n",
    "raw_data_length = 0 #\n",
    "\n",
    "def set_globvar_to_one():\n",
    "    global num_ch    # Needed to modify global copy of globvar\n",
    "    num_ch = 16\n",
    "    global raw_data_length    # Needed to modify global copy of globvar\n",
    "    raw_data_length = 400*60*10\n",
    "\n",
    "def print_globvar():\n",
    "    print(num_ch,raw_data_length)     # No need for global declaration to read value of globvar\n",
    "\n",
    "set_globvar_to_one()\n",
    "print_globvar()       # Prints 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code read raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of file found: 444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mypath = './sample_data/train_1'\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "file_path = []\n",
    "for x in f:\n",
    "    file_path.extend([dirpath+'/'+x])\n",
    "ttnf = len(file_path)\n",
    "print('total number of file found:',ttnf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample_data/train_1/1_53_1.mat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-273-93dadde1f67e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataStruct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mraw_sig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     np.append(raw_sig,raw_data[0][0][0],2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "raw_sig = np.zeros((raw_data_length,num_ch,ttnf))\n",
    "for i,fp in enumerate(file_path):\n",
    "    print(fp)\n",
    "    raw_data = loadmat(fp)['dataStruct']\n",
    "    raw_sig[:,:,i] = raw_data[0][0][0]\n",
    "#     np.append(raw_sig,raw_data[0][0][0],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code handles the case when all channel returns zero\n",
    "Lynn is going to do this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "raw_sig.shape\n",
    "sig = raw_sig[:,0,0]\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(sig)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code extract features from signal using wavelet transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def single_channel_dwt(sig):\n",
    "#     input 240000\n",
    "    coeffs = pywt.wavedec(sig, 'db1', level=6)\n",
    "    cA2, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
    "    return cD6\n",
    "\n",
    "def multi_channel_dwt(sig):\n",
    "#     input: 240000 * 16\n",
    "#     return: k * 16\n",
    "    F = []\n",
    "    for s_sig in sig.T:\n",
    "        cd = single_channel_dwt(s_sig)\n",
    "        F.append(cd)\n",
    "#     return np.asarray(F)\n",
    "    return F\n",
    "\n",
    "def feature_extraction(sig):\n",
    "#     input: n * 240000 * 16\n",
    "#     return: #files *channels * features\n",
    "    FF = []\n",
    "    for i in range(sig.shape[2]):\n",
    "        F = multi_channel_dwt(raw_sig[:,:,1])\n",
    "        FF.append(F)\n",
    "    return FF\n",
    "\n",
    "print(raw_sig.shape)\n",
    "FF = feature_extraction(raw_sig) \n",
    "\n",
    "Y = []\n",
    "for i,F in enumerate(FF):\n",
    "    tmp = np.asarray(F)\n",
    "    X = np.reshape(tmp,tmp.shape[0]*tmp.shape[1])\n",
    "    Y.append(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code do dim reduction\n",
    "Deepak going to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = np.asarray(Y)\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = model.fit_transform(X) \n",
    "print(T)\n",
    "plt.plot(T[:,0],T[:,1],'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code do clustering \n",
    "Deepak can futher improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
